---
title: "Topic 6: Decision Trees"
author: "Chiraag Potharaju"
date: "2023-09-30"
categories: [news, code, analysis]
image: "Decision-Trees-2.png"
---

This post gives detailed descriptions and analysis about Decision Trees

<br>

**Introduction to Decision Trees**

Decision trees are very powerful and a versatile machine learning technique used for both classification and regression tasks. Throughout this blog post, we will explore decision trees, discussing their fundamental concepts and how they can be applied various real-world problems. Using Python, Scikit-learn, Tensorflow, graphs, executable code blocks, and other visualization tools, we will show the different features as well as advantages of decision trees.

<br>

**Importance of Decision Trees**

Decision trees are hierarchical structures with the purpose of mimicking human decision-making processes. They consists of nodes representing decision and branches that symbolize different outcomes, and at the bottom, we are left with leafs, which represent final decisions. These trees can be used or a wide range of tasks such as diagnosis medical conditions or predicting customer behavior. Some advantages include:

-   Easy to explain with a clear visualization of the process.

-   Can handle both numerical and categorical data.

-   Very little data preprocessing, meaning it can handle missing data or outliers.

-   No assumptions are made about the shape or distribution of data.

<br>

**Step 0:**

This step is to ensure that required libraries are installed on the machine before proceeding.

```         
---   pip install numpy pandas matplotlib scikit-learn ---
```

This step installs the required libraries to run the code below. Ensure python is installed on the machine already and the terminal has admin access when running this command.

<br>

**Step 1:**

When using Python, we have to import some libraries that will be utilized throughout the blog.

```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.tree import plot_tree
from sklearn.preprocessing import StandardScaler
```

Some of the imports include numpy and pandas for data manipulation, sklearn modules from training and evaluation, and DecisionTreeClassifier for building the model.

<br>

**Step 2:**

Import the data set

```{python}
from sklearn.datasets import load_iris  

iris_data = load_iris() 
X = iris_data.data 
y = iris_data.target
```

This code imports the iris data set and loads the data and target values into variables.

<br>

**Step 3:**

We can peek the data by plotting a scatter-plot matrix.

```{python}
iris_df = pd.DataFrame(iris_data.data, columns=iris_data.feature_names)
iris_df['target'] = iris_data.target

pd.plotting.scatter_matrix(iris_df.drop('target', axis=1), c=iris_df['target'], figsize=(10, 8))
plt.show()
```

By executing the commands above, we are given a scatter-plot matrix that gives a visualization of the relationships in the data.

<br>

**Step 4:**

Next, we will split the data into training and testing sets

```{python}
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

We used a test_size of 0.2 meaning that 20% of the data will be allocated for testing and 80% will be used for training. The rest of the variables are different data and target points being inserted.

<br>

**Step 5:**

After dividing the data, we will train a Decision Tree classifier.

```{python}
model = DecisionTreeClassifier() 
model.fit(X_train, y_train)
```

This code fits the model on the training data.

<br>

**Step 6:**

Evaluate the test set

```{python}
predictions = model.predict(X_test) 
accuracy = accuracy_score(y_test, predictions)  

print("Accuracy:", accuracy)
```

This code helps to evaluate the data and print out the accuracy values.

<br>

**Step 7:**

The following code is to visualize the decision tree containing the data.

```{python}
plt.figure(figsize=(12, 8))
plot_tree(model, filled=True, feature_names=iris_data.feature_names, class_names=iris_data.target_names)
plt.show()
```

The trees structure and nodes are displayed above, making it more visible as to the computations that were done and how every decision was made.

<br>

**Step 8:**

If we want to make predictions on new data, we need to simulate new points, add the data, and tell the scalar to make the appropriate predictions.

This is example of a data set.

```{python}

new_data = np.array([[5.1, 3.8, 1.5, 0.3]]) 

scaler = StandardScaler()
X = scaler.fit_transform(X)
new_data = scaler.transform(new_data) 

model = DecisionTreeClassifier()
model.fit(X_train, y_train)

new_predictions = model.predict(new_data)

print("Predicted class for custom data:", iris_data.target_names[new_predictions[0]])

```

As we can see, when using those specific values, the model predicts that the class of the entered data is virginica.

Below, we will use a different data set and see the results.

```{python}

new_data = np.array([[0.1, 0.8, 0.5, 0.3]]) 

scaler = StandardScaler()
X = scaler.fit_transform(X)
new_data = scaler.transform(new_data) 

model = DecisionTreeClassifier()
model.fit(X_train, y_train)

new_predictions = model.predict(new_data)

print("Predicted class for custom data:", iris_data.target_names[new_predictions[0]])
```

As we can see, when using those specific values, the model predicts that the class of the entered data is versicolor. This is a different prediction from the custom values we used originally, showing how the model is parsing through the different options of the decision tree.

<br>

**Conclusion:**

As we can see, decision trees are a very effective and efficient tool. Once the model is trained on the data, it can then make several decisions and predictions for various classification and regression purposes. In the future, decision trees will continue to get stronger and more accurate, allowing data analysts to make more decisions.
