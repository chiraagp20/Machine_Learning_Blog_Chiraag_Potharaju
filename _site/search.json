[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning Blog",
    "section": "",
    "text": "Topic 6: Decision Trees\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nSep 30, 2023\n\n\nChiraag Potharaju\n\n\n\n\n\n\n  \n\n\n\n\nTopic 5: Anomaly & Outlier Detection\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nSep 29, 2023\n\n\nChiraag Potharaju\n\n\n\n\n\n\n  \n\n\n\n\nTopic 4: Classification\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nSep 28, 2023\n\n\nChiraag Potharaju\n\n\n\n\n\n\n  \n\n\n\n\nTopic 3: Linear & Nonlinear Regression\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nSep 27, 2023\n\n\nChiraag Potharaju\n\n\n\n\n\n\n  \n\n\n\n\nTopic 2: Clustering\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nSep 26, 2023\n\n\nChiraag Potharaju\n\n\n\n\n\n\n  \n\n\n\n\nTopic 1: Probability Theory and Random Variables\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nSep 25, 2023\n\n\nChiraag Potharaju\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nSep 22, 2023\n\n\nChiraag Potharaju\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Classification/index.html",
    "href": "posts/Classification/index.html",
    "title": "Topic 1: Classification",
    "section": "",
    "text": "This post gives detailed descriptions and analysis about Classification.\n\nIntroduction to Classification\nClassification is a machine learning task that is primarily used in predicting categorical labels or classes. Throughout this post, we will dive deeper into classification in machine learning using Python, Scikit-learn, and Tensorflow highlighting the importance of this concept. There will also be several data visualizations and executable code chunks to emphasize these points.\n\nImportance of Classification\nThere are numerous applications for classification, some of which include:\n\nEmails- Classifying emails as spam or not\nFraud- Classifying monetary transactions as fraud or not\nMedical- Classifying symptoms/images into categories to make accurate predictions on disease/issue\nSpeech Recognition- Virtual assistants classify spoken words to respond accordingly\n\nThese are just a few of the many application of classification used throughout daily life.\n\nStep 0:\nThis step is to ensure that required libraries are installed on the machine before proceeding.\n---\n  pip install numpy pandas matplotlib scikit-learn\n---\nThis step installs the required libraries to run the code below. Ensure python is installed on the machine already and the terminal has admin access when running this command.\n\nStep 1:\nWhen using Python, we have to import some libraries that will be utilized throughout the Classification process.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\n\nSome of the imports include numpy and pandas for data manipulation, sklearn modules from training and evaluation, and LogisticRegression for building the model.\n\nStep 2:\nImport the data set\n\nfrom sklearn.datasets import load_iris\n\niris_data = load_iris()\nX = iris_data.data\ny = iris_data.target\n\nThis code imports the iris data set and loads the data and target values into variables.\n\nStep 3:\nWe can peek the data by plotting a scatter-plot matrix.\n\niris_df = pd.DataFrame(iris_data.data, columns=iris_data.feature_names)\niris_df['target'] = iris_data.target\n\npd.plotting.scatter_matrix(iris_df.drop('target', axis=1), c=iris_df['target'], figsize=(10,8))\nplt.show()\n\n\n\n\nBy executing the commands above, we are given a scatter-plot matrix that gives a visualization of the relationships in the data.\n\nStep 4:\nNext, we will split the data into training and testing sets\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nWe used a test_size of 0.2 meaning that 20% of the data will be allocated for testing and 80% will be used for training. The rest of the variables are different data and target points being inserted.\n\nStep 5:\nAfter dividing the data, we will train a Logistic Regression classifier.\n\nmodel = LogisticRegression(max_iter = 1000)\nmodel.fit(X_train, y_train)\n\nLogisticRegression(max_iter=1000)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(max_iter=1000)\n\n\nThis code fits the model on the training data.\n\nStep 6:\nEvaluate the test set\n\npredictions = model.predict(X_test)\naccuracy = accuracy_score(y_test, predictions)\n\nprint(\"Accuracy:\", accuracy)\n\nAccuracy: 1.0\n\n\nThis code helps to evaluate the data and print out the accuracy values.\n\nStep 7:\nThe following code is to visualize the confusion matrix of the data.\n\n# Compute confusion matrix\nconf_mat = confusion_matrix(y_test, predictions)\n\n# Plot confusion matrix\nplt.imshow(conf_mat, interpolation='nearest')\nplt.title(\"Confusion Matrix\")\nplt.colorbar()\nplt.show()\n\n\n\n\nThis gives us more insight into the performance across classes.\n\nStep 8:\nIf we want to make predictions on new data, we need to simulate new points, add the data, and tell the scalar to make the appropriate predictions.\nThis is example of a data set.\n\n# Standardize features\nscaler = StandardScaler()  \nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Train model\nmodel = LogisticRegression(max_iter=1000, solver='lbfgs')\nmodel.fit(X_train, y_train)\n\n# New data point  \nnew_x = [[5.1, 3.8, 1.5, 0.3]]\nnew_x = scaler.transform(new_x)\n\n# Probabilistic predictions\nprobs = model.predict_proba(new_x)\n\n# Print predictions\nprint('Prediction Probabilities:', probs)\n\nPrediction Probabilities: [[9.89916261e-01 1.00836487e-02 9.07712609e-08]]\n\n\nAs the output shows, once the new data was added, there were predictions made, however, the new data wasn’t great, therefore the prediction values were very low, almost 0, meaning this would not be good data to have.\n\nHowever, with a different data set, we can see the prediction values are vastly different.\n\n# Standardize features\nscaler = StandardScaler()  \nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Train model\nmodel = LogisticRegression(max_iter=1000, solver='lbfgs')\nmodel.fit(X_train, y_train)\n\n# New data point  \nnew_x = [[0.1, 0.2, 0.5, 0.3]]\nnew_x = scaler.transform(new_x)\n\n# Probabilistic predictions\nprobs = model.predict_proba(new_x)\n\n# Print predictions\nprint('Prediction Probabilities:', probs)\n\nPrediction Probabilities: [[0.04031464 0.81970355 0.13998181]]\n\n\nHowever, once this data was tested, the numeric values outputted are clearly higher, showing that this data set is much more accurate and more practical when comparing to the original data set.\n\nConclusion:\nAs shown throughout this post, the use of classification is highly beneficial into grouping data into different categories and making predictions based on previous data inputted. Once the model is trained on prior data, the data can then make fairly accurate predictions on the data entered and the validity and accuracy in comparison to the original values. Classification is a highly beneficial topic for Machine Learning and it will continue to become more advanced and accurate in the future."
  },
  {
    "objectID": "posts/Introduction/index.html",
    "href": "posts/Introduction/index.html",
    "title": "Introduction",
    "section": "",
    "text": "Welcome!\nMy name is Chiraag Potharaju and I am currently a graduate student enrolled at Virginia Tech taking CS 5804: Introduction to Machine Learning I.\n\nThe purpose of this blog is to go in depth about 3 separate topics related to Machine Learning. Throughout the 3 posts in this blog, there will be detailed descriptions, code, and graphs and other diagrams to show various visualizations of these topics.\nIf there are any questions or issues regarding the content in these posts, feel free to contact me at chiraagp20@vt.edu."
  },
  {
    "objectID": "Machine_Learning_Blog_Chiraag_Potharaju-3d7488a33f60fdd79f39342051cad1e86145b81a/about.html",
    "href": "Machine_Learning_Blog_Chiraag_Potharaju-3d7488a33f60fdd79f39342051cad1e86145b81a/about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "Machine_Learning_Blog_Chiraag_Potharaju-3d7488a33f60fdd79f39342051cad1e86145b81a/index.html",
    "href": "Machine_Learning_Blog_Chiraag_Potharaju-3d7488a33f60fdd79f39342051cad1e86145b81a/index.html",
    "title": "Machine Learning Blog",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "posts/Decision Trees/index.html",
    "href": "posts/Decision Trees/index.html",
    "title": "Topic 2: Decision Trees",
    "section": "",
    "text": "This post gives detailed descriptions and analysis about Decision Trees\n\nIntroduction to Decision Trees\nDecision trees are very powerful and a versatile machine learning technique used for both classification and regression tasks. Throughout this blog post, we will explore decision trees, discussing their fundamental concepts and how they can be applied various real-world problems. Using Python, Scikit-learn, Tensorflow, graphs, executable code blocks, and other visualization tools, we will show the different features as well as advantages of decision trees.\n\nImportance of Decision Trees\nDecision trees are hierarchical structures with the purpose of mimicking human decision-making processes. They consists of nodes representing decision and branches that symbolize different outcomes, and at the bottom, we are left with leafs, which represent final decisions. These trees can be used or a wide range of tasks such as diagnosis medical conditions or predicting customer behavior. Some advantages include:\n\nEasy to explain with a clear visualization of the process.\nCan handle both numerical and categorical data.\nVery little data preprocessing, meaning it can handle missing data or outliers.\nNo assumptions are made about the shape or distribution of data.\n\n\nStep 0:\nThis step is to ensure that required libraries are installed on the machine before proceeding.\n---   pip install numpy pandas matplotlib scikit-learn ---\nThis step installs the required libraries to run the code below. Ensure python is installed on the machine already and the terminal has admin access when running this command.\n\nStep 1:\nWhen using Python, we have to import some libraries that will be utilized throughout the process.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import plot_tree\nfrom sklearn.preprocessing import StandardScaler\n\nSome of the imports include numpy and pandas for data manipulation, sklearn modules from training and evaluation, and DecisionTreeClassifier for building the model.\n\nStep 2:\nImport the data set\n\nfrom sklearn.datasets import load_iris  \n\niris_data = load_iris() \nX = iris_data.data \ny = iris_data.target\n\nThis code imports the iris data set and loads the data and target values into variables.\n\nStep 3:\nWe can peek the data by plotting a scatter-plot matrix.\n\niris_df = pd.DataFrame(iris_data.data, columns=iris_data.feature_names)\niris_df['target'] = iris_data.target\n\npd.plotting.scatter_matrix(iris_df.drop('target', axis=1), c=iris_df['target'], figsize=(10, 8))\nplt.show()\n\n\n\n\nBy executing the commands above, we are given a scatter-plot matrix that gives a visualization of the relationships in the data.\n\nStep 4:\nNext, we will split the data into training and testing sets\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nWe used a test_size of 0.2 meaning that 20% of the data will be allocated for testing and 80% will be used for training. The rest of the variables are different data and target points being inserted.\n\nStep 5:\nAfter dividing the data, we will train a Decision Tree classifier.\n\nmodel = DecisionTreeClassifier() \nmodel.fit(X_train, y_train)\n\nDecisionTreeClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifierDecisionTreeClassifier()\n\n\nThis code fits the model on the training data.\n\nStep 6:\nEvaluate the test set\n\npredictions = model.predict(X_test) \naccuracy = accuracy_score(y_test, predictions)  \n\nprint(\"Accuracy:\", accuracy)\n\nAccuracy: 1.0\n\n\nThis code helps to evaluate the data and print out the accuracy values.\n\nStep 7:\nThe following code is to visualize the decision tree containing the data.\n\nplt.figure(figsize=(12, 8))\nplot_tree(model, filled=True, feature_names=iris_data.feature_names, class_names=iris_data.target_names)\nplt.show()\n\n\n\n\nThe trees structure and nodes are displayed above, making it more visible as to the computations that were done and how every decision was made.\n\nStep 8:\nIf we want to make predictions on new data, we need to simulate new points, add the data, and tell the scalar to make the appropriate predictions.\nThis is example of a data set.\n\nnew_data = np.array([[5.1, 3.8, 1.5, 0.3]]) \n\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\nnew_data = scaler.transform(new_data) \n\nmodel = DecisionTreeClassifier()\nmodel.fit(X_train, y_train)\n\nnew_predictions = model.predict(new_data)\n\nprint(\"Predicted class for custom data:\", iris_data.target_names[new_predictions[0]])\n\nPredicted class for custom data: setosa\n\n\nAs we can see, when using those specific values, the model predicts that the class of the entered data is virginica.\nBelow, we will use a different data set and see the results.\n\nnew_data = np.array([[0.1, 0.8, 0.5, 0.3]]) \n\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\nnew_data = scaler.transform(new_data) \n\nmodel = DecisionTreeClassifier()\nmodel.fit(X_train, y_train)\n\nnew_predictions = model.predict(new_data)\n\nprint(\"Predicted class for custom data:\", iris_data.target_names[new_predictions[0]])\n\nPredicted class for custom data: setosa\n\n\nAs we can see, when using those specific values, the model predicts that the class of the entered data is versicolor. This is a different prediction from the custom values we used originally, showing how the model is parsing through the different options of the decision tree.\n\nConclusion:\nAs we can see, decision trees are a very effective and efficient tool. Once the model is trained on the data, it can then make several decisions and predictions for various classification and regression purposes. In the future, decision trees will continue to get stronger and more accurate, allowing data analysts to make more decisions."
  },
  {
    "objectID": "posts/Probability Theory & Random Variables/index.html",
    "href": "posts/Probability Theory & Random Variables/index.html",
    "title": "Topic 1: Probability Theory and Random Variables",
    "section": "",
    "text": "This post gives detailed descriptions and analysis about Probability Theory and Random Variables.\n\nIntroduction to Probability Theory and Random Variables\nProbability Theory is a framework to analyze random events which allow us to quantify likelihood and uncertainty. Throughout this post, we will dive deeper into probability theory in machine learning using Python, Scikit-learn, and Tensorflow highlighting the importance of this concept. There will also be several data visualizations and executable code chunks to emphasize these points.\n\nImportant Theories and Variables\nThere are a some types of variables, distributions, and variables that are important to understand for a basis. Some of them include:\n\nRandom Variable- A variable whose value depends on the outcome of a random event. Can be discrete or continuous. Ex: Dice roll or coin flip.\nDiscrete Random Variable- A random variable with a countable number of possible values.\nContinuous Random Variable- A random variable with an uncountable number of possible values.\nNormal Distribution- A “bell curve” distribution. Mean and variance are used with this.\nBinomial Distribution- Illustrates the number of successes in independent yes/no experiments. Each experiment would have a p chance of succeeding.\nPoisson Distribution- Illustrates the number of events occurring in a fixed set of time.\nLaw of Large Numbers- The sample mean comes closer to population mean as more values are accounted for.\nCentral Limit Theorem- Sum of random variables converge to normal distribution.\nBayes’ Theorem- Relates conditional and marginal probabilities through a formula.\n\nThese theories, distributions, and variables are used in several fields such as statistics, machine learning finance, engineering, medicine, science, and more.\n\nStep 0:\nThis step is to ensure that required libraries are installed on the machine before proceeding.\n---   pip install numpy matplotlib scikit-learn ---\nThis step installs the required libraries to run the code below. Ensure python is installed on the machine already and the terminal has admin access when running this command.\n\nStep 1:\nWhen using Python, we have to import some libraries that will be utilized throughout the Classification process.\n\nimport numpy as np \nimport matplotlib.pyplot as plt \nimport scipy.stats as stats \nfrom scipy.stats import binom\nfrom scipy.stats import norm\nfrom scipy.stats import poisson\n\nSome of the imports include numpy for data manipulation, matplotlib for data visualization, and scipy stats for statistical comparisons.\n\nStep 2:\nIllustrate a binomial distribution.\n\n# Simulate 10 coin flips  \nflips = binom.rvs(n=10, p=0.5, size=10000)  \n\n# Plot binomial histogram\nplt.hist(flips, bins=11, density=True) \n\n# Overlay theoretical distribution\nx = np.arange(11)\nplt.plot(x, stats.binom.pmf(x, 10, 0.5))\n\nplt.title(\"Binomial Distribution\")\n\nText(0.5, 1.0, 'Binomial Distribution')\n\n\n\n\n\nThis code does 10 random coin flips and creates a chart to illustrate the binomial distribution over the theoretical distribution.\n\nStep 3:\nIllustrate a normal distribution.\n\n# Generate normal data\ndata = norm.rvs(size=1000) \n\n# Plot histogram\nplt.hist(data, bins=100, density=True)\nplt.title(\"Normal Distribution\")\nplt.show()\n\n\n\n\nBy executing the commands above, we are shown an example of a normal distribution. As well can see in the graph, there is a bell-curve like structure, with the middle of the graph being the highest and the sides of the curve being the lowest, meaning the middle values occurred the most and dropped off towards the sides.\n\nStep 4:\nIllustrate a Poisson distribution.\n\n# Generate Poisson-distributed data with mean of 5\ndata = poisson.rvs(mu=5, size=10000)\n\n# Plot the histogram\nplt.hist(data, bins=20, density=True)\nplt.title(\"Poisson Distribution (mean=5)\")\nplt.xlabel(\"Value\") \nplt.ylabel(\"Probability\")\n\n# Add vertical line at mean\nplt.axvline(poisson.mean(5), color='red') \n\n# Show plot\nplt.show()\n\n\n\n\nBy running this code, we can see an example of a poisson distribution. The histogram shows that around the values of 4-8 we see the highest probability, and after that, there is a consistent decrease.\n\nStep 5:\nIllustrate the Law of Large Numbers.\n\n# Simulate 10 coin flips\nflips1 = np.random.binomial(1, 0.5, 10)\nmean1 = flips1.mean()\n\n# Simulate 1000 coin flips\nflips2 = np.random.binomial(1, 0.5, 1000) \nmean2 = flips2.mean()\n\n# Plot histograms\nplt.subplot(1,2,1)\nplt.title(\"10 Flips\")\nplt.hist(flips1)\nplt.axvline(mean1, color='r')\n\nplt.subplot(1,2,2)\nplt.title(\"1000 Flips\")\nplt.hist(flips2)\nplt.axvline(mean2, color='r')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nWhen examining the first graph, with 10 coin flips, we notice the lopsidedness between the outcomes, favoring one side over the other heavily. However, in the second graph, with a 1000 flips, we notice that the bars are about even. Additionally, in the first graph, the mean line is at 0.6, showing it favoring one side over another, but in the second graph, the mean line is at 0.5, showing that it is just about even. This illustrates the Law of Large Numbers, showing that the more data there is, the closer to the assumed mean the data will be.\n\nStep 6:\nIllustrate the Central Limit Theorem.\n\ndata = np.random.exponential(scale=2, size=10000)\n\nmeans = []\nfor i in range(10, len(data), 30):\n    sample = data[:i]\n    means.append(sample.mean())\n\nplt.plot(means)\nplt.xlabel(\"Sample Size\") \nplt.ylabel(\"Sample Mean\")\nplt.title(\"Convergence of Sample Mean\")\n\nText(0.5, 1.0, 'Convergence of Sample Mean')\n\n\n\n\n\nAs we see in the graph, as the data points are slowly being summed together over time, we get closer to a normal distribution. There is a very erratic start and it slowly changes to a straighter data set.\n\nStep 7:\nThe following code is to visualize the confusion matrix of the data.\n\n# Prior belief - 90% chance of rain\np_rain = 0.9  \n\n# Likelihood - test is 90% accurate\np_correct = 0.9\n\n# Observation - positive test\np_pos_given_rain = 0.9 \np_pos_given_norain = 0.1\n\n# Posterior - chance of rain given + test   \np_rain_given_pos = (p_pos_given_rain * p_rain) / ((p_pos_given_rain * p_rain) + p_pos_given_norain * (1 - p_rain))\n\nprint(p_rain_given_pos)\n\n0.9878048780487805\n\n\nAfter analyzing this code, we can see the formula, and data points inserted. According to Bayes’ Theorem, if there is 90% chance of rain and the test is 90% accurate, there is a 98.7% chance of rain.\n\nConclusion:\nAs shown throughout this post, the use of Probability Theorems and Random Variables is important in creating predictions and analyzing current data. Through different formulas, graphs, and illustrations, we are able to visualize the different data sets and make educated predictions and assumptions about the data. By using these concepts in Machine Learning, the machine has the ability to implement these formulas and make accurate guesses too. Throughout this blog post, only a few topics were explored, however, there are dozens more out there that all serve their own individual purpose."
  },
  {
    "objectID": "posts/Clustering/index.html",
    "href": "posts/Clustering/index.html",
    "title": "Topic 2: Clustering",
    "section": "",
    "text": "This post gives detailed descriptions and analysis about Clustering.\n\nIntroduction to Clustering\nClustering a machine learning technique used to group data points together based on similarities. It can reveal patterns and other common themes to allow accurate predictions and assumptions to be made. Throughout this post, we will dive deeper into probability theory in machine learning using Python, Scikit-learn, and Tensorflow highlighting the importance of this concept. There will also be several data visualizations and executable code chunks to emphasize these points.\n\nImportant Algorithms and Values\nThere are a some types of algorithms and variables that are important to understand for a basis. Some of them include:\n\nK-Means Clustering- Groups data into k number of groups. Efficient for large data sets.\nHierarchical Clustering- Builds a hierarchy of clusters using either top-down or bottom-up manners.\nDBSCAN- Finds clusters based on density. Can detect arbitrary shapes and ignores outliers.\nGaussian Mixture Models- Uses a probabilistic model to assume data comes from from a mixture of Gaussian Distributions. Soft clustering.\nSilhouette coefficient- Quantifies how tightly groups data points are within a cluster to other clusters. Ranges from -1 to 1.\nCalinski-Harabasz Index- Computes ratio of cluster-to-cluster dispersion, as well as inter cluster dispersion. Higher values claim they are dense.\nDavies-Bouldin- Calculates average similarity between each cluster and its most similar one. Lower value means more separation between clusters.\n\nThese algorithms and variables are used in several fields such as statistics, machine learning, finance, engineering, medicine, science, and more.\n\nStep 0:\nThis step is to ensure that required libraries are installed on the machine before proceeding.\n---   pip install numpy matplotlib scikit-learn ---\nThis step installs the required libraries to run the code below. Ensure python is installed on the machine already and the terminal has admin access when running this command.\n\nStep 1:\nWhen using Python, we have to import some libraries that will be utilized throughout the Classification process.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.metrics import calinski_harabasz_score\nfrom sklearn.metrics import davies_bouldin_score\nfrom scipy.cluster.hierarchy import linkage\nfrom scipy.cluster.hierarchy import dendrogram\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.mixture import GaussianMixture\n\nSome of the imports include numpy for data manipulation, matplotlib for data visualization, and different sklearn libraries to illustrate different clustering terms.\n\nStep 2:\nIllustrate K-Means Clustering.\n\n# Generate sample data\nX, y = make_blobs(n_samples=500, n_features=2, centers=4, cluster_std=1.8, random_state=101)\n\n# K-means clustering\nkmeans = KMeans(n_clusters=4, n_init=10)\nkmeans.fit(X)\ny_kmeans = kmeans.predict(X)\n\n# Plot k-means clusters\nplt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\ncenters = kmeans.cluster_centers_\nplt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.9)\nplt.show()\n\n# Evaluate clusters\nprint(\"Silhouette Coefficient: \", silhouette_score(X, y_kmeans))\nprint(\"Calinski-Harabasz Index: \", calinski_harabasz_score(X, y_kmeans)) \nprint(\"Davies-Bouldin Index: \", davies_bouldin_score(X, y_kmeans))\n\n\n\n\nSilhouette Coefficient:  0.5494869398680785\nCalinski-Harabasz Index:  1384.0339615400558\nDavies-Bouldin Index:  0.6150394374843552\n\n\nAs the graph above displays, there are 4 clear groups of data points, as separated by the colors. The silhouette coefficient score of 0.549 indicates that the clusters are well separated and compact. It could be better, but it is still decent. The Calinski-Harabasz Index score of 1384 indicates that data there is good density and separation, as well as good cluster validity. The Davies-Bouldin Index score of 0.615 indicates that that there is moderate separation between clusters with a little overlap. Users can make valid assumptions based on just the scores, however, when looking at the graph, we can see the assumptions made from the scores line up exactly with how the graphs look. These values are very important for statisticians to make their predictions.\n\nStep 3:\nIllustrate Hierarchical clustering.\n\n# Generate sample data\nX = [[1,1], [3,4], [4,7], [3,5], [5,6], [6,6], [9,10]]\n\n# Perform hierarchical clustering\nlinked = linkage(X, 'single') \n\n# Plot the dendrogram\ndendrogram(linked,\n            orientation='top',\n            distance_sort='descending',\n            show_leaf_counts=True)\n\nplt.show()\n\n\n\n\nWhen examining the graph above, there are several evident clusters that were created. There are 6 clear clusters that are shown. some data points are very similar, such as 3 and 1 and 4 and 5. However, some data points such as 2 and 6 are way farther apart, showing a lack of similarity between those points. This dendrogram essentially displays how close points are to each other, or similarity, with the distance between the values being the key marker.\n\nStep 4:\nIllustrate DBSCAN clustering.\n\n# Generate sample data\nX = np.array([[1, 2], [2, 2], [2, 3],\n              [8, 7], [8, 8], [20,20], [25, 80]])\n\n# Perform DBSCAN clustering\ndbscan = DBSCAN(eps=3, min_samples=2)\ny_dbscan = dbscan.fit_predict(X)\n\n# Plot clusters \nplt.scatter(X[:, 0], X[:, 1], c=y_dbscan)\nplt.title(\"DBSCAN\")\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\nplt.show()\n\n\n\n\nWhen looking at the graph above, we can see several points that are grouped together based on color. The reason that they are grouped together is because of proximity to other points. There are core points there, and the other points are chosen based on their proximity to those points, hence the reason the grouping is as it is.\n\nStep 5:\nIllustrate Gaussian Mixture Models.\n\n# Generate sample data\nX = np.array([[1, 2], \n              [1.5, 2.5], \n              [3, 2], \n              [10, 20],\n              [11, 25],\n              [12, 22]])\n\n# Fit Gaussian Mixture Model\ngmm = GaussianMixture(n_components=3)\ngmm.fit(X)\n\n# Predict cluster labels \ny_gmm = gmm.predict(X)\n\n# Plot clusters\nplt.scatter(X[:,0], X[:,1], c=y_gmm, s=40, cmap='viridis')\n\nplt.show()\n\n\n\n\nWhen analyzing the data points, we can clearly see 3 different colors for the different data points. The yellow group is could be based on the similarity on high y-axis values, the purple with high x-axis values, while the turquoise group is based on low x and y axis values. This graph can be useful to a viewer because if they were adding another point, they can take a valid guess as how it would be grouped.\n\nConclusion:\nAs shown throughout this post, the use of Clustering is important in creating predictions and analyzing current data. Through different algorithms and variables, we are able to visualize the different data sets and make educated predictions and assumptions about the data. There were several graphs that were created, and by analyzing those, we could see the potential of creating predictions for future data points. The practical uses of clustering are endless and it will continue to get more efficient and accurate. By using these concepts in Machine Learning, the machine has the ability to implement these formulas and make accurate guesses too. Throughout this blog post, only a few topics were explored, however, there are dozens more out there that all serve their own individual purpose."
  },
  {
    "objectID": "posts/Linear & Nonlinear Regression/index.html",
    "href": "posts/Linear & Nonlinear Regression/index.html",
    "title": "Topic 3: Linear & Nonlinear Regression",
    "section": "",
    "text": "This post gives detailed descriptions and analysis about Linear and Nonlinear Regression.\n\nIntroduction to Linear and Nonlinear Regression\nLinear and Nonlinear Regression is a special graphical analysis to analyze trends in data to create educated guesses on future data. Throughout this post, we will dive deeper into probability theory in machine learning using Python, Scikit-learn, and Tensorflow highlighting the importance of this concept. There will also be several data visualizations and executable code chunks to emphasize these points.\n\nImportant Theories and Variables\nThere are a some important algorithms that are important to understand. Some of them, for Linear Regression, include:\n\nOrdinary Least Squares\nGradient Descent\nLasso and Ridge Regression\n\nSome nonlinear regression techniques include:\n\nPolynomial Regression\nSupport Vector Regression\nNeural Networks\n\nThese different techniques serve several different purposes in real life such as predicting house prices, forecasting sales, estimating risks, modeling processes, approximating function, or several other purposes.\n\nStep 0:\nThis step is to ensure that required libraries are installed on the machine before proceeding.\n---   pip install numpy matplotlib scikit-learn tensorflow ---\nThis step installs the required libraries to run the code below. Ensure python is installed on the machine already and the terminal has admin access when running this command.\n\nStep 1:\nWhen using Python, we have to import some libraries that will be utilized throughout the blog.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.svm import SVR\n\nSome of the imports include numpy for data manipulation, matplotlib for data visualization, and sklearn libraries for statistical visualizations and formulas.\n\nStep 2:\nIllustrate the Ordinary Least Squares Linear Regression.\n\nX = [[1], [2], [3], [4], [5]]\ny = [1, 3, 2, 3, 5]\n\nols = LinearRegression()\nols.fit(X, y)\ny_ols_pred = ols.predict(X)\n\nplt.scatter(X, y)\nplt.plot(X, y_ols_pred, color='red', label='OLS')\n\nprint(\"Coefficients:\", ols.coef_)\nprint(\"Intercept:\", ols.intercept_)\n\nCoefficients: [0.8]\nIntercept: 0.3999999999999999\n\n\n\n\n\nWhen analyzing the graph and the values that were outputted, there are several assumptions that can be made. Using the y = mx + b formula, since this is a linear regression formula, we can see that the intercept is 0.3999. That means that when x is 0, the y value will be 0.3999. the coefficient is 0.8, which means that is the m value, meaning that it is multiplied with whatever the x value is at the moment, then added to 0.3999. We can see there is a positive trend in the graph, meaning that we can assume that the higher the x value, the higher the y value.\n\nStep 3:\nIllustrate the Gradient Descent Linear Regression.\n\n# Define the data\nX = np.array([[1], [3], [4], [1], [2]])\ny = np.array([1, 3, 2, 3, 5])\n\n# Initialize the coefficients (slope and intercept) with random values\ntheta0 = np.random.rand()\ntheta1 = np.random.rand()\n\n# Set the learning rate and the number of iterations\nlearning_rate = 0.01\nnum_iterations = 1000\n\n# Perform gradient descent\nfor i in range(num_iterations):\n    # Calculate the predictions\n    y_pred = theta0 + theta1 * X\n    \n    # Calculate the errors\n    error = y_pred - y\n    \n    # Update the coefficients using the gradient\n    theta0 -= learning_rate * (1/len(X)) * np.sum(error)\n    theta1 -= learning_rate * (1/len(X)) * np.sum(error * X)\n    \n# Calculate predictions using the final coefficients\ny_gd_pred = theta0 + theta1 * X\n\n# Plot the original data and the regression line\nplt.scatter(X, y)\nplt.plot(X, y_gd_pred, color='green', label='Gradient Descent')\n\nprint(\"Coefficients:\", theta1)\nprint(\"Intercept:\", theta0)\n\nplt.legend()\nplt.show()\n\nCoefficients: 4.8789469662310274e-05\nIntercept: 2.7998668074695385\n\n\n\n\n\nWhen examining the graph, we can clearly see two values that are printed out, a coefficient, and an intercept. Similar to the OLS, we use a y = mx + b formula for a linear regression. The coefficient here is 0.0004, meaning that there is a very small increase. Additionally, the intercept is 2.799, meaning that when x is 0, the y value is 2.799. The green gradient descent line represents the slope, showing the relationship between the x and y values. As we can see here, there is very little correlation between the data points, but since this has to be a linear regression, it attempts to create a model as best as possible.\n\nStep 4:\nIllustrate the Lass and Ridge Regression.\n\n# Create some sample data\nX = np.array([[1], [2], [3], [4], [5]])\ny = np.array([1, 3, 2, 3, 5])\n\n# Create Lasso and Ridge regression models\nlasso = Lasso(alpha=1.0)  # You can adjust the alpha (regularization strength)\nridge = Ridge(alpha=1.0)  # You can adjust the alpha (regularization strength)\n\n# Fit the models to the data\nlasso.fit(X, y)\nridge.fit(X, y)\n\n# Make predictions\ny_lasso_pred = lasso.predict(X)\ny_ridge_pred = ridge.predict(X)\n\n# Plot the original data, Lasso regression line, and Ridge regression line\nplt.scatter(X, y, label='Data')\nplt.plot(X, y_lasso_pred, color='blue', label='Lasso Regression')\nplt.plot(X, y_ridge_pred, color='green', label='Ridge Regression')\n\nplt.legend()\nplt.show()\n\n\n\n\nWhen analyzing the graph, we can see the ridge slope being much greater than the the lasso slope, meaning it has a higher coefficient, however, the lasso slope has a higher intercept. The lasso regression helps to encourage sparsity, meanwhile, the ridge focuses on stabilizing the model by reducing the coefficients. Lasso helps to deal with only the relevant data points while ridge works to maintain all features. Together, both these regression models work together to create a strong model that can help the user create an accurate assumption on future data.\n\n___________________________________________________________________________________________________________\nThe three theorems shown above are all linear regressions, as shown by the straight slope lines and the consistency when using the y = mx + b formula. The following 3 algorithms are nonlinear algorithms, where they do not follow a straight formula and will rarely be a straight line.\n___________________________________________________________________________________________________________\n\nStep 5:\nIllustrate Polynomial Regression.\n\n# Generate some sample data\nnp.random.seed(0)\nX = np.random.rand(100, 1) * 10\ny = 3 * X + X**2 + np.random.randn(100, 1)\n\n# Create polynomial features (in this case, a second-degree polynomial)\npoly_features = PolynomialFeatures(degree=2)\nX_poly = poly_features.fit_transform(X)\n\n# Create a linear regression model\npoly_regression = LinearRegression()\n\n# Fit the model to the polynomial features\npoly_regression.fit(X_poly, y)\n\n# Generate predictions\nX_new = np.linspace(0, 10, 100).reshape(-1, 1)\nX_new_poly = poly_features.transform(X_new)\ny_new = poly_regression.predict(X_new_poly)\n\n# Plot the original data and the polynomial regression line\nplt.scatter(X, y, label='Data')\nplt.plot(X_new, y_new, color='red', label='Polynomial Regression')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.legend()\nplt.show()\n\n\n\n\nWhen examining the graph above, there is a clear model that demonstrates a polynomial regression. Compared to the graphs above, this regression line is curved, showing the polynomial relationship. The data points here were chosen for the most part to create a clean data set but it clearly shows the relationship and how there is no longer a y = mx + b formula, as the coefficient is now squared.\n\nStep 6:\nIllustrate Support Vector Regression.\n\n# Generate some sample data\nnp.random.seed(0)\nX = np.sort(5 * np.random.rand(80, 1), axis=0)\ny = np.sin(X).ravel()\n\n# Add noise to the data\ny[::5] += 3 * (0.5 - np.random.rand(16))\n\n# Create the SVR model\nsvr = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=0.2)\n\n# Fit the model to the data\nsvr.fit(X, y)\n\n# Generate predictions\nX_new = np.linspace(0, 5, 100)[:, np.newaxis]\ny_svr = svr.predict(X_new)\n\n# Plot the data points and the SVR line\nplt.scatter(X, y, color='darkorange', label='Data')\nplt.plot(X_new, y_svr, color='navy', lw=2, label='SVR')\n\n# Customize the plot\nplt.title('Support Vector Regression')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.legend()\nplt.show()\n\n\n\n\nAs we see in the graph, there is a clear regression line that separates the data. Similar to the polynomial regression, the trend line is not straight, as it is more of a wave pattern. The line separates the data somewhat well and creates a strong line in terms of how close it is to the data points.\n\nStep 7:\nIllustrate Neural Networks.\n\n# Sample data\nX = np.array([[0,0],[0,1],[1,0],[1,1]])\ny = np.array([0,1,1,0]) \n\n# Hyperparameters\nlearning_rate = 0.1  \nepochs = 1000\n\n# Initialize weights randomly\nnp.random.seed(1)\nweights = np.random.rand(2,4)\n\nfor epoch in range(epochs):\n\n    # Forward pass\n    outputs = X.dot(weights)\n    \n    # Calculate loss\n    loss = np.square(outputs - y).sum()\n    \n    # Backpropagation\n    grad = 2*X.T.dot(outputs - y)\n\n    # Take average over samples\n    grad = grad.mean(axis=0, keepdims=True) \n\n    # Update weights  \n    weights -= learning_rate * grad\n\n# Plot decision boundary   \nplt.scatter(X[:,0], X[:,1], c=y, s=100, cmap='winter')\nax = plt.gca()\nax.set_xlim([-0.1,1.1])\nax.set_ylim([-0.1,1.1])\n\n# Plot decision boundary\nx = np.linspace(-0.1, 1.1, 100)\ny = -weights[0,0] / weights[1,0] * x\nplt.plot(x, y)\n\nplt.title(\"Decision Boundary\")\nplt.show()\n\nprint(weights)\n\n\n\n\n[[ 0.13513306  0.98065962  0.57359375 -0.02161408]\n [-0.13513306  0.35267372  0.75973958  0.02161408]]\n\n\nWhen making the comparisons and analyzing the outputs, we can see that the weights for input 0 isn’t as significant, 0.13, as input 1, 0.98. this means that input 1 contributes to the output more positively.\n\nConclusion:\nAs shown throughout this post, the use of different theorems in relation to linear and nonlinear regression is important in creating predictions and analyzing current data. Through different formulas, graphs, and illustrations, we are able to visualize the different data sets and make educated predictions and assumptions about the data. By using these concepts in Machine Learning, the machine has the ability to implement these formulas and make accurate guesses too. Throughout this blog post, only a few types of regressions were explored, however, there are dozens more out there that all serve their own individual purpose."
  },
  {
    "objectID": "posts/Anomaly & Outlier Detection/index.html",
    "href": "posts/Anomaly & Outlier Detection/index.html",
    "title": "Topic 5: Anomaly & Outlier Detection",
    "section": "",
    "text": "This post gives detailed descriptions and analysis about Anomaly & Outlier Detection\n\nIntroduction to Anomaly and Outlier Detection\nAnomaly & Outlier Detection is a concept in machine learning where different graphs are analyzed for data that seems “different” or wrong compared to the other data. Throughout this post, we will dive deeper into Anomaly and Outlier Detection in machine learning using Python, Scikit-learn, and Tensorflow highlighting the importance of this concept. There will also be several data visualizations and executable code chunks to emphasize these points.\n\nImportance of Anomaly & Outlier Detection\nThe detection of these unusual variables is very important as it validates the data and ensures the analysis can be done correctly. If there are a couple points that are way off from the bulk of the data, it causes the analyst to question the data and wonder why that occurred and if the data points should still be considered. The points could mess up trend lines and other predictions, so it is important that these points are found and dealt with correctly.\n\nStep 0:\nThis step is to ensure that required libraries are installed on the machine before proceeding.\n---\n  pip install numpy pandas matplotlib scikit-learn\n---\nThis step installs the required libraries to run the code below. Ensure python is installed on the machine already and the terminal has admin access when running this command.\n\nStep 1:\nWhen using Python, we have to import some libraries that will be utilized throughout the blog.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.linear_model import LinearRegression\n\nSome of the imports include numpy and pandas for data manipulation, sklearn modules from training and evaluation, and LocalOutlierFactor for analysis.\n\nStep 2:\nImport the data set. We then peek the data in a scatter-plot matrix.\n\ndata = load_breast_cancer()\nX = data.data\ny = data.target\n\n# Add outlier points   \nnum_outliers = 10\nrandom_indices = np.random.choice(len(X), num_outliers)\nfor i in random_indices:\n    X[i] += np.random.normal(size=X.shape[1]) * 100\n\nplt.scatter(X[:,0], X[:,1])\nplt.title('Breast Cancer Data')\nplt.xlabel('Mean radius')  \nplt.ylabel('Mean texture')\nplt.show()\n\n\n\n\nThis code imports the breast cancer data set and loads the data and target values into variables.\n\nStep 3:\nNext, we will fit the data.\n\nlof = LocalOutlierFactor()\nlof.fit_predict(X)\n\narray([ 1,  1,  1, -1,  1, -1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1, -1, -1,  1,  1,  1,  1,  1,  1, -1,  1,  1,\n        1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1, -1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n        1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,\n        1,  1, -1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,  1, -1, -1,\n        1,  1, -1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1, -1,  1,\n        1,  1, -1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1, -1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,\n        1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,\n        1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,\n        1,  1, -1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n        1,  1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1])\n\n\n\nStep 4:\nAfter fitting the data, we try to find the outliers.\n\nscores = -lof.negative_outlier_factor_\n\nthreshold = 3.25\noutliers = scores &gt; threshold\nnum_outliers = outliers.sum()\n\nprint(f'{num_outliers} outliers detected.')\n\n8 outliers detected.\n\n\nThis code prints the outliers detected. Clearly, there were 10 outliers that were detected in the data above.\n\nStep 6:\nShow data set with outliers\n\nplt.scatter(X[:,0], X[:,1], c=outliers)\nplt.title(\"Outlier Detection\")\nplt.show()\n\n\n\n\nThis code helps to evaluate the data illustrate which values are treated as the outliers. The yellow points in the graph are the outliers, while the purple set illustrates the majority of the data. We can clearly see that the yellow points are separated, hence making it more obvious that they are the outliers. There also appear to be some anomalies, the group of random points towards the corners.\n\nStep 7:\nThe following code is to show the different formulas.\n\ndata = load_breast_cancer()\n\nX = data.data  \ny = data.target\n\n# Add outlier points    \nnum_outliers = 10\nrandom_indices = np.random.choice(len(X), num_outliers)\nfor i in random_indices:\n    X[i] += np.random.normal(size=X.shape[1]) * 100\n    \n# Regression with outliers\nlr = LinearRegression().fit(X, y)\nprint(\"Regression line with outliers:\")\nprint(\"y =\", lr.coef_[0], \"* x +\", lr.intercept_)\n\n# Regression without outliers \nX_filtered = np.delete(X, random_indices, axis=0)\ny_filtered = np.delete(y, random_indices)\nlr.fit(X_filtered, y_filtered)\nprint(\"Regression line without outliers:\") \nprint(\"y =\", lr.coef_[0], \"* x +\", lr.intercept_)\n\nRegression line with outliers:\ny = 0.021187315648114515 * x + 2.710600674221939\nRegression line without outliers:\ny = 0.19513516909472778 * x + 3.071764769641227\n\n\nWhen analyzing the formulas, we can see that they are drastically different, especially with one having a positive slope, and the other being a negative slope. This is all relevant on how the user deems the importance of these rogue data points.\n\nConclusion:\nAs shown throughout this post, highlighting outliers and anomalies is very important in realizing the validity and accuracy of the data. If there are too many rogue points, there could have been some faulty testing or something else was messed up along the say. Choosing what data points are outliers or not is very important, and it is vital that when training the machine learning model, it can accurately identify them. Once the model is trained on prior data, the data can then make fairly accurate predictions on the data entered and the validity and accuracy in comparison to the original values. This is a very important topic in terms of graphical analysis and for accurate forecasting, and as time progressing, it will be done better. In this post, the most basic level of detection was done, but it does highlight the importance of this topic."
  }
]