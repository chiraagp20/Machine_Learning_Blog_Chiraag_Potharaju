{
  "hash": "6812c1a4f25f6a5a05041c439ce0fe80",
  "result": {
    "markdown": "---\ntitle: \"Topic 1: Classification\"\nauthor: \"Chiraag Potharaju\"\ndate: \"2023-09-25\"\ncategories: [news, code, analysis]\nimage: \"Classification_pic1.png\"\n---\n\nThis post gives detailed descriptions and analysis about Classification.\n\n<br>\n\n**Introduction to Classification**\n\nClassification is a machine learning task that is primarily used in predicting categorical labels or classes. Throughout this post, we will dive deeper into classification in machine learning using Python, Scikit-learn, and Tensorflow highlighting the importance of this concept. There will also be several data visualizations and executable code chunks to emphasize these points.\n\n<br>\n\n**Importance of Classification**\n\nThere are numerous applications for classification, some of which include:\n\n-   Emails- Classifying emails as spam or not\n\n-   Fraud- Classifying monetary transactions as fraud or not\n\n-   Medical- Classifying symptoms/images into categories to make accurate predictions on disease/issue\n\n-   Speech Recognition- Virtual assistants classify spoken words to respond accordingly\n\nThese are just a few of the many application of classification used throughout daily life.\n\n<br>\n\n**Step 0:**\n\nThis step is to ensure that required libraries are installed on the machine before proceeding.\n\n```         \n---\n  pip install numpy pandas matplotlib scikit-learn\n---\n```\n\nThis step installs the required libraries to run the code below. Ensure python is installed on the machine already and the terminal has admin access when running this command.\n\n<br>\n\n**Step 1:**\n\nWhen using Python, we have to import some libraries that will be utilized throughout the Classification process.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\n```\n:::\n\n\nSome of the imports include numpy and pandas for data manipulation, sklearn modules from training and evaluation, and LogisticRegression for building the model.\n\n<br>\n\n**Step 2:**\n\nImport the data set\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nfrom sklearn.datasets import load_iris\n\niris_data = load_iris()\nX = iris_data.data\ny = iris_data.target\n```\n:::\n\n\nThis code imports the iris data set and loads the data and target values into variables.\n\n<br>\n\n**Step 3:**\n\nWe can peek the data by plotting a scatter-plot matrix.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\niris_df = pd.DataFrame(iris_data.data, columns=iris_data.feature_names)\niris_df['target'] = iris_data.target\n\npd.plotting.scatter_matrix(iris_df.drop('target', axis=1), c=iris_df['target'], figsize=(10,8))\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){width=798 height=645}\n:::\n:::\n\n\nBy executing the commands above, we are given a scatter-plot matrix that gives a visualization of the relationships in the data.\n\n<br>\n\n**Step 4:**\n\nNext, we will split the data into training and testing sets\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n```\n:::\n\n\nWe used a test_size of 0.2 meaning that 20% of the data will be allocated for testing and 80% will be used for training. The rest of the variables are different data and target points being inserted.\n\n<br>\n\n**Step 5:**\n\nAfter dividing the data, we will train a Logistic Regression classifier.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nmodel = LogisticRegression(max_iter = 1000)\nmodel.fit(X_train, y_train)\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```{=html}\n<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div>\n```\n:::\n:::\n\n\nThis code fits the model on the training data.\n\n<br>\n\n**Step 6:**\n\nEvaluate the test set\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\npredictions = model.predict(X_test)\naccuracy = accuracy_score(y_test, predictions)\n\nprint(\"Accuracy:\", accuracy)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy: 1.0\n```\n:::\n:::\n\n\nThis code helps to evaluate the data and print out the accuracy values.\n\n<br>\n\n**Step 7:**\n\nThe following code is to visualize the confusion matrix of the data.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n# Compute confusion matrix\nconf_mat = confusion_matrix(y_test, predictions)\n\n# Plot confusion matrix\nplt.imshow(conf_mat, interpolation='nearest')\nplt.title(\"Confusion Matrix\")\nplt.colorbar()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-1.png){width=501 height=431}\n:::\n:::\n\n\nThis gives us more insight into the performance across classes.\n\n<br>\n\n**Step 8:**\n\nIf we want to make predictions on new data, we need to simulate new points, add the data, and tell the scalar to make the appropriate predictions.\n\nThis is example of a data set.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n# Standardize features\nscaler = StandardScaler()  \nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Train model\nmodel = LogisticRegression(max_iter=1000, solver='lbfgs')\nmodel.fit(X_train, y_train)\n\n# New data point  \nnew_x = [[5.1, 3.8, 1.5, 0.3]]\nnew_x = scaler.transform(new_x)\n\n# Probabilistic predictions\nprobs = model.predict_proba(new_x)\n\n# Print predictions\nprint('Prediction Probabilities:', probs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPrediction Probabilities: [[9.89916261e-01 1.00836487e-02 9.07712609e-08]]\n```\n:::\n:::\n\n\nAs the output shows, once the new data was added, there were predictions made, however, the new data wasn't great, therefore the prediction values were very low, almost 0, meaning this would not be good data to have.\n\n<br>\n\nHowever, with a different data set, we can see the prediction values are vastly different.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\n# Standardize features\nscaler = StandardScaler()  \nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Train model\nmodel = LogisticRegression(max_iter=1000, solver='lbfgs')\nmodel.fit(X_train, y_train)\n\n# New data point  \nnew_x = [[0.1, 0.2, 0.5, 0.3]]\nnew_x = scaler.transform(new_x)\n\n# Probabilistic predictions\nprobs = model.predict_proba(new_x)\n\n# Print predictions\nprint('Prediction Probabilities:', probs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPrediction Probabilities: [[0.04031464 0.81970355 0.13998181]]\n```\n:::\n:::\n\n\nHowever, once this data was tested, the numeric values outputted are clearly higher, showing that this data set is much more accurate and more practical when comparing to the original data set.\n\n<br>\n\n**Conclusion:**\n\nAs shown throughout this post, the use of classification is highly beneficial into grouping data into different categories and making predictions based on previous data inputted. Once the model is trained on prior data, the data can then make fairly accurate predictions on the data entered and the validity and accuracy in comparison to the original values. Classification is a highly beneficial topic for Machine Learning and it will continue to become more advanced and accurate in the future.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}