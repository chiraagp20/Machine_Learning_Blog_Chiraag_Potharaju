{
  "hash": "d28e78eac4718c715a6b0b354873d29b",
  "result": {
    "markdown": "---\ntitle: \"Topic 5: Anomaly & Outlier Detection\"\nauthor: \"Chiraag Potharaju\"\ndate: \"2023-09-29\"\ncategories: [news, code, analysis]\nimage: \"outlier.png\"\n---\n\nThis post gives detailed descriptions and analysis about Anomaly & Outlier Detection\n\n<br>\n\n**Introduction to Anomaly and Outlier Detection**\n\nAnomaly & Outlier Detection is a concept in machine learning where different graphs are analyzed for data that seems \"different\" or wrong compared to the other data. Throughout this post, we will dive deeper into Anomaly and Outlier Detection in machine learning using Python, Scikit-learn, and Tensorflow highlighting the importance of this concept. There will also be several data visualizations and executable code chunks to emphasize these points.\n\n<br>\n\n**Importance of Anomaly & Outlier Detection**\n\nThe detection of these unusual variables is very important as it validates the data and ensures the analysis can be done correctly. If there are a couple points that are way off from the bulk of the data, it causes the analyst to question the data and wonder why that occurred and if the data points should still be considered. The points could mess up trend lines and other predictions, so it is important that these points are found and dealt with correctly.\n\n<br>\n\n**Step 0:**\n\nThis step is to ensure that required libraries are installed on the machine before proceeding.\n\n```         \n---\n  pip install numpy pandas matplotlib scikit-learn\n---\n```\n\nThis step installs the required libraries to run the code below. Ensure python is installed on the machine already and the terminal has admin access when running this command.\n\n<br>\n\n**Step 1:**\n\nWhen using Python, we have to import some libraries that will be utilized throughout the blog.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.linear_model import LinearRegression\n```\n:::\n\n\nSome of the imports include numpy and pandas for data manipulation, sklearn modules from training and evaluation, and LocalOutlierFactor for analysis.\n\n<br>\n\n**Step 2:**\n\nImport the data set. We then peek the data in a scatter-plot matrix.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndata = load_breast_cancer()\nX = data.data\ny = data.target\n\n# Add outlier points   \nnum_outliers = 10\nrandom_indices = np.random.choice(len(X), num_outliers)\nfor i in random_indices:\n    X[i] += np.random.normal(size=X.shape[1]) * 100\n\nplt.scatter(X[:,0], X[:,1])\nplt.title('Breast Cancer Data')\nplt.xlabel('Mean radius')  \nplt.ylabel('Mean texture')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=604 height=449}\n:::\n:::\n\n\nThis code imports the breast cancer data set and loads the data and target values into variables.\n\n<br>\n\n**Step 3:**\n\nNext, we will fit the data.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nlof = LocalOutlierFactor()\nlof.fit_predict(X)\n```\n\n::: {.cell-output .cell-output-display execution_count=87}\n```\narray([ 1,  1,  1, -1,  1, -1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1, -1, -1,  1,  1,  1,  1,  1,  1, -1,  1,  1,\n        1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1, -1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n        1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,\n        1,  1, -1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,  1, -1, -1,\n        1,  1, -1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1, -1,  1,\n        1,  1, -1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1, -1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,\n        1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,\n        1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,\n        1,  1, -1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n        1,  1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1])\n```\n:::\n:::\n\n\n<br>\n\n**Step 4:**\n\nAfter fitting the data, we try to find the outliers.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nscores = -lof.negative_outlier_factor_\n\nthreshold = 3.25\noutliers = scores > threshold\nnum_outliers = outliers.sum()\n\nprint(f'{num_outliers} outliers detected.')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n8 outliers detected.\n```\n:::\n:::\n\n\nThis code prints the outliers detected. Clearly, there were 10 outliers that were detected in the data above.\n\n<br>\n\n**Step 6:**\n\nShow data set with outliers\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nplt.scatter(X[:,0], X[:,1], c=outliers)\nplt.title(\"Outlier Detection\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-1.png){width=586 height=431}\n:::\n:::\n\n\nThis code helps to evaluate the data illustrate which values are treated as the outliers. The yellow points in the graph are the outliers, while the purple set illustrates the majority of the data. We can clearly see that the yellow points are separated, hence making it more obvious that they are the outliers. There also appear to be some anomalies, the group of random points towards the corners.\n\n<br>\n\n**Step 7:**\n\nThe following code is to show the different formulas.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\ndata = load_breast_cancer()\n\nX = data.data  \ny = data.target\n\n# Add outlier points    \nnum_outliers = 10\nrandom_indices = np.random.choice(len(X), num_outliers)\nfor i in random_indices:\n    X[i] += np.random.normal(size=X.shape[1]) * 100\n    \n# Regression with outliers\nlr = LinearRegression().fit(X, y)\nprint(\"Regression line with outliers:\")\nprint(\"y =\", lr.coef_[0], \"* x +\", lr.intercept_)\n\n# Regression without outliers \nX_filtered = np.delete(X, random_indices, axis=0)\ny_filtered = np.delete(y, random_indices)\nlr.fit(X_filtered, y_filtered)\nprint(\"Regression line without outliers:\") \nprint(\"y =\", lr.coef_[0], \"* x +\", lr.intercept_)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRegression line with outliers:\ny = 0.021187315648114515 * x + 2.710600674221939\nRegression line without outliers:\ny = 0.19513516909472778 * x + 3.071764769641227\n```\n:::\n:::\n\n\nWhen analyzing the formulas, we can see that they are drastically different, especially with one having a positive slope, and the other being a negative slope. This is all relevant on how the user deems the importance of these rogue data points.\n\n<br>\n\n**Conclusion:**\n\nAs shown throughout this post, highlighting outliers and anomalies is very important in realizing the validity and accuracy of the data. If there are too many rogue points, there could have been some faulty testing or something else was messed up along the say. Choosing what data points are outliers or not is very important, and it is vital that when training the machine learning model, it can accurately identify them. Once the model is trained on prior data, the data can then make fairly accurate predictions on the data entered and the validity and accuracy in comparison to the original values. This is a very important topic in terms of graphical analysis and for accurate forecasting, and as time progressing, it will be done better. In this post, the most basic level of detection was done, but it does highlight the importance of this topic.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}