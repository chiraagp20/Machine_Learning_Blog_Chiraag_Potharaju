{
  "hash": "c58e9cd0ebe7790b0dfb1dc733bb6e10",
  "result": {
    "markdown": "---\ntitle: \"Topic 3: Linear & Nonlinear Regression\" \nauthor: \"Chiraag Potharaju\" \ndate: \"2023-09-27\" \ncategories: [news, code, analysis] \nimage: \"Regression.png\" \n---\n\nThis post gives detailed descriptions and analysis about Linear and Nonlinear Regression.\n\n<br>\n\n**Introduction to Linear and Nonlinear Regression**\n\nLinear and Nonlinear Regression is a special graphical analysis to analyze trends in data to create educated guesses on future data. Throughout this post, we will dive deeper into probability theory in machine learning using Python, Scikit-learn, and Tensorflow highlighting the importance of this concept. There will also be several data visualizations and executable code chunks to emphasize these points.\n\n<br>\n\n**Important Theories and Variables**\n\nThere are a some important algorithms that are important to understand. Some of them, for Linear Regression, include:\n\n-   Ordinary Least Squares\n\n-   Gradient Descent\n\n-   Lasso and Ridge Regression\n\nSome nonlinear regression techniques include:\n\n-   Polynomial Regression\n\n-   Support Vector Regression\n\n-   Neural Networks\n\nThese different techniques serve several different purposes in real life such as predicting house prices, forecasting sales, estimating risks, modeling processes, approximating function, or several other purposes.\n\n<br>\n\n**Step 0:**\n\nThis step is to ensure that required libraries are installed on the machine before proceeding.\n\n```         \n---   pip install numpy matplotlib scikit-learn tensorflow ---\n```\n\nThis step installs the required libraries to run the code below. Ensure python is installed on the machine already and the terminal has admin access when running this command.\n\n<br>\n\n**Step 1:**\n\nWhen using Python, we have to import some libraries that will be utilized throughout the blog.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.svm import SVR\n```\n:::\n\n\nSome of the imports include numpy for data manipulation, matplotlib for data visualization, and sklearn libraries for statistical visualizations and formulas.\n\n<br>\n\n**Step 2:**\n\nIllustrate the Ordinary Least Squares Linear Regression.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nX = [[1], [2], [3], [4], [5]]\ny = [1, 3, 2, 3, 5]\n\nols = LinearRegression()\nols.fit(X, y)\ny_ols_pred = ols.predict(X)\n\nplt.scatter(X, y)\nplt.plot(X, y_ols_pred, color='red', label='OLS')\n\nprint(\"Coefficients:\", ols.coef_)\nprint(\"Intercept:\", ols.intercept_)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCoefficients: [0.8]\nIntercept: 0.3999999999999999\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-2.png){width=571 height=411}\n:::\n:::\n\n\nWhen analyzing the graph and the values that were outputted, there are several assumptions that can be made. Using the y = mx + b formula, since this is a linear regression formula, we can see that the intercept is 0.3999. That means that when x is 0, the y value will be 0.3999. the coefficient is 0.8, which means that is the m value, meaning that it is multiplied with whatever the x value is at the moment, then added to 0.3999. We can see there is a positive trend in the graph, meaning that we can assume that the higher the x value, the higher the y value.\n\n<br>\n\n**Step 3:**\n\nIllustrate the Gradient Descent Linear Regression.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# Define the data\nX = np.array([[1], [3], [4], [1], [2]])\ny = np.array([1, 3, 2, 3, 5])\n\n# Initialize the coefficients (slope and intercept) with random values\ntheta0 = np.random.rand()\ntheta1 = np.random.rand()\n\n# Set the learning rate and the number of iterations\nlearning_rate = 0.01\nnum_iterations = 1000\n\n# Perform gradient descent\nfor i in range(num_iterations):\n    # Calculate the predictions\n    y_pred = theta0 + theta1 * X\n    \n    # Calculate the errors\n    error = y_pred - y\n    \n    # Update the coefficients using the gradient\n    theta0 -= learning_rate * (1/len(X)) * np.sum(error)\n    theta1 -= learning_rate * (1/len(X)) * np.sum(error * X)\n    \n# Calculate predictions using the final coefficients\ny_gd_pred = theta0 + theta1 * X\n\n# Plot the original data and the regression line\nplt.scatter(X, y)\nplt.plot(X, y_gd_pred, color='green', label='Gradient Descent')\n\nprint(\"Coefficients:\", theta1)\nprint(\"Intercept:\", theta0)\n\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCoefficients: 4.8789469662310274e-05\nIntercept: 2.7998668074695385\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-2.png){width=571 height=411}\n:::\n:::\n\n\nWhen examining the graph, we can clearly see two values that are printed out, a coefficient, and an intercept. Similar to the OLS, we use a y = mx + b formula for a linear regression. The coefficient here is 0.0004, meaning that there is a very small increase. Additionally, the intercept is 2.799, meaning that when x is 0, the y value is 2.799. The green gradient descent line represents the slope, showing the relationship between the x and y values. As we can see here, there is very little correlation between the data points, but since this has to be a linear regression, it attempts to create a model as best as possible.\n\n<br>\n\n**Step 4:**\n\nIllustrate the Lass and Ridge Regression.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# Create some sample data\nX = np.array([[1], [2], [3], [4], [5]])\ny = np.array([1, 3, 2, 3, 5])\n\n# Create Lasso and Ridge regression models\nlasso = Lasso(alpha=1.0)  # You can adjust the alpha (regularization strength)\nridge = Ridge(alpha=1.0)  # You can adjust the alpha (regularization strength)\n\n# Fit the models to the data\nlasso.fit(X, y)\nridge.fit(X, y)\n\n# Make predictions\ny_lasso_pred = lasso.predict(X)\ny_ridge_pred = ridge.predict(X)\n\n# Plot the original data, Lasso regression line, and Ridge regression line\nplt.scatter(X, y, label='Data')\nplt.plot(X, y_lasso_pred, color='blue', label='Lasso Regression')\nplt.plot(X, y_ridge_pred, color='green', label='Ridge Regression')\n\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){width=571 height=411}\n:::\n:::\n\n\nWhen analyzing the graph, we can see the ridge slope being much greater than the the lasso slope, meaning it has a higher coefficient, however, the lasso slope has a higher intercept. The lasso regression helps to encourage sparsity, meanwhile, the ridge focuses on stabilizing the model by reducing the coefficients. Lasso helps to deal with only the relevant data points while ridge works to maintain all features. Together, both these regression models work together to create a strong model that can help the user create an accurate assumption on future data.\n\n<br>\n\n\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n\nThe three theorems shown above are all linear regressions, as shown by the straight slope lines and the consistency when using the y = mx + b formula. The following 3 algorithms are nonlinear algorithms, where they do not follow a straight formula and will rarely be a straight line.\n\n\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n\n<br>\n\n**Step 5:**\n\nIllustrate Polynomial Regression.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# Generate some sample data\nnp.random.seed(0)\nX = np.random.rand(100, 1) * 10\ny = 3 * X + X**2 + np.random.randn(100, 1)\n\n# Create polynomial features (in this case, a second-degree polynomial)\npoly_features = PolynomialFeatures(degree=2)\nX_poly = poly_features.fit_transform(X)\n\n# Create a linear regression model\npoly_regression = LinearRegression()\n\n# Fit the model to the polynomial features\npoly_regression.fit(X_poly, y)\n\n# Generate predictions\nX_new = np.linspace(0, 10, 100).reshape(-1, 1)\nX_new_poly = poly_features.transform(X_new)\ny_new = poly_regression.predict(X_new_poly)\n\n# Plot the original data and the polynomial regression line\nplt.scatter(X, y, label='Data')\nplt.plot(X_new, y_new, color='red', label='Polynomial Regression')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-1.png){width=593 height=429}\n:::\n:::\n\n\nWhen examining the graph above, there is a clear model that demonstrates a polynomial regression. Compared to the graphs above, this regression line is curved, showing the polynomial relationship. The data points here were chosen for the most part to create a clean data set but it clearly shows the relationship and how there is no longer a y = mx + b formula, as the coefficient is now squared.\n\n<br>\n\n**Step 6:**\n\nIllustrate Support Vector Regression.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n# Generate some sample data\nnp.random.seed(0)\nX = np.sort(5 * np.random.rand(80, 1), axis=0)\ny = np.sin(X).ravel()\n\n# Add noise to the data\ny[::5] += 3 * (0.5 - np.random.rand(16))\n\n# Create the SVR model\nsvr = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=0.2)\n\n# Fit the model to the data\nsvr.fit(X, y)\n\n# Generate predictions\nX_new = np.linspace(0, 5, 100)[:, np.newaxis]\ny_svr = svr.predict(X_new)\n\n# Plot the data points and the SVR line\nplt.scatter(X, y, color='darkorange', label='Data')\nplt.plot(X_new, y_svr, color='navy', lw=2, label='SVR')\n\n# Customize the plot\nplt.title('Support Vector Regression')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-1.png){width=600 height=449}\n:::\n:::\n\n\nAs we see in the graph, there is a clear regression line that separates the data. Similar to the polynomial regression, the trend line is not straight, as it is more of a wave pattern. The line separates the data somewhat well and creates a strong line in terms of how close it is to the data points.\n\n<br>\n\n**Step 7:**\n\nIllustrate Neural Networks.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n# Sample data\nX = np.array([[0,0],[0,1],[1,0],[1,1]])\ny = np.array([0,1,1,0]) \n\n# Hyperparameters\nlearning_rate = 0.1  \nepochs = 1000\n\n# Initialize weights randomly\nnp.random.seed(1)\nweights = np.random.rand(2,4)\n\nfor epoch in range(epochs):\n\n    # Forward pass\n    outputs = X.dot(weights)\n    \n    # Calculate loss\n    loss = np.square(outputs - y).sum()\n    \n    # Backpropagation\n    grad = 2*X.T.dot(outputs - y)\n\n    # Take average over samples\n    grad = grad.mean(axis=0, keepdims=True) \n\n    # Update weights  \n    weights -= learning_rate * grad\n\n# Plot decision boundary   \nplt.scatter(X[:,0], X[:,1], c=y, s=100, cmap='winter')\nax = plt.gca()\nax.set_xlim([-0.1,1.1])\nax.set_ylim([-0.1,1.1])\n\n# Plot decision boundary\nx = np.linspace(-0.1, 1.1, 100)\ny = -weights[0,0] / weights[1,0] * x\nplt.plot(x, y)\n\nplt.title(\"Decision Boundary\")\nplt.show()\n\nprint(weights)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-1.png){width=571 height=431}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[[ 0.13513306  0.98065962  0.57359375 -0.02161408]\n [-0.13513306  0.35267372  0.75973958  0.02161408]]\n```\n:::\n:::\n\n\nWhen making the comparisons and analyzing the outputs, we can see that the weights for input 0 isn't as significant, 0.13, as input 1, 0.98. this means that input 1 contributes to the output more positively.\n\n<br>\n\n**Conclusion:**\n\nAs shown throughout this post, the use of different theorems in relation to linear and nonlinear regression is important in creating predictions and analyzing current data. Through different formulas, graphs, and illustrations, we are able to visualize the different data sets and make educated predictions and assumptions about the data. By using these concepts in Machine Learning, the machine has the ability to implement these formulas and make accurate guesses too. Throughout this blog post, only a few types of regressions were explored, however, there are dozens more out there that all serve their own individual purpose.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}