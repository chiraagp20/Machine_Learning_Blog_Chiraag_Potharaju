{
  "hash": "72386294483f51fee8ab5216a0498b88",
  "result": {
    "markdown": "---\ntitle: \"Topic 1: Probability Theory and Random Variables\" \nauthor: \"Chiraag Potharaju\" \ndate: \"2023-09-25\" \ncategories: [news, code, analysis] \nimage: \"probability.jpg\" \n---\n\nThis post gives detailed descriptions and analysis about Probability Theory and Random Variables.\n\n<br>\n\n**Introduction to Probability Theory and Random Variables**\n\nProbability Theory is a framework to analyze random events which allow us to quantify likelihood and uncertainty. Throughout this post, we will dive deeper into probability theory in machine learning using Python, Scikit-learn, and Tensorflow highlighting the importance of this concept. There will also be several data visualizations and executable code chunks to emphasize these points.\n\n<br>\n\n**Important Theories and Variables**\n\nThere are a some types of variables, distributions, and variables that are important to understand for a basis. Some of them include:\n\n-   Random Variable- A variable whose value depends on the outcome of a random event. Can be discrete or continuous. Ex: Dice roll or coin flip.\n\n-   Discrete Random Variable- A random variable with a countable number of possible values.\n\n-   Continuous Random Variable- A random variable with an uncountable number of possible values.\n\n-   Normal Distribution- A \"bell curve\" distribution. Mean and variance are used with this.\n\n-   Binomial Distribution- Illustrates the number of successes in independent yes/no experiments. Each experiment would have a p chance of succeeding.\n\n-   Poisson Distribution- Illustrates the number of events occurring in a fixed set of time.\n\n-   Law of Large Numbers- The sample mean comes closer to population mean as more values are accounted for.\n\n-   Central Limit Theorem- Sum of random variables converge to normal distribution.\n\n-   Bayes' Theorem- Relates conditional and marginal probabilities through a formula.\n\nThese theories, distributions, and variables are used in several fields such as statistics, machine learning finance, engineering, medicine, science, and more.\n\n<br>\n\n**Step 0:**\n\nThis step is to ensure that required libraries are installed on the machine before proceeding.\n\n```         \n---   pip install numpy matplotlib scikit-learn ---\n```\n\nThis step installs the required libraries to run the code below. Ensure python is installed on the machine already and the terminal has admin access when running this command.\n\n<br>\n\n**Step 1:**\n\nWhen using Python, we have to import some libraries that will be utilized throughout the Classification process.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np \nimport matplotlib.pyplot as plt \nimport scipy.stats as stats \nfrom scipy.stats import binom\nfrom scipy.stats import norm\nfrom scipy.stats import poisson\n```\n:::\n\n\nSome of the imports include numpy for data manipulation, matplotlib for data visualization, and scipy stats for statistical comparisons.\n\n<br>\n\n**Step 2:**\n\nIllustrate a binomial distribution.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Simulate 10 coin flips  \nflips = binom.rvs(n=10, p=0.5, size=10000)  \n\n# Plot binomial histogram\nplt.hist(flips, bins=11, density=True) \n\n# Overlay theoretical distribution\nx = np.arange(11)\nplt.plot(x, stats.binom.pmf(x, 10, 0.5))\n\nplt.title(\"Binomial Distribution\")\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\nText(0.5, 1.0, 'Binomial Distribution')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-2.png){width=579 height=431}\n:::\n:::\n\n\nThis code does 10 random coin flips and creates a chart to illustrate the binomial distribution over the theoretical distribution.\n\n<br>\n\n**Step 3:**\n\nIllustrate a normal distribution.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# Generate normal data\ndata = norm.rvs(size=1000) \n\n# Plot histogram\nplt.hist(data, bins=100, density=True)\nplt.title(\"Normal Distribution\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){width=571 height=431}\n:::\n:::\n\n\nBy executing the commands above, we are shown an example of a normal distribution. As well can see in the graph, there is a bell-curve like structure, with the middle of the graph being the highest and the sides of the curve being the lowest, meaning the middle values occurred the most and dropped off towards the sides.\n\n<br>\n\n**Step 4:**\n\nIllustrate a Poisson distribution.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# Generate Poisson-distributed data with mean of 5\ndata = poisson.rvs(mu=5, size=10000)\n\n# Plot the histogram\nplt.hist(data, bins=20, density=True)\nplt.title(\"Poisson Distribution (mean=5)\")\nplt.xlabel(\"Value\") \nplt.ylabel(\"Probability\")\n\n# Add vertical line at mean\nplt.axvline(poisson.mean(5), color='red') \n\n# Show plot\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){width=597 height=449}\n:::\n:::\n\n\nBy running this code, we can see an example of a poisson distribution. The histogram shows that around the values of 4-8 we see the highest probability, and after that, there is a consistent decrease.\n\n<br>\n\n**Step 5:**\n\nIllustrate the Law of Large Numbers.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# Simulate 10 coin flips\nflips1 = np.random.binomial(1, 0.5, 10)\nmean1 = flips1.mean()\n\n# Simulate 1000 coin flips\nflips2 = np.random.binomial(1, 0.5, 1000) \nmean2 = flips2.mean()\n\n# Plot histograms\nplt.subplot(1,2,1)\nplt.title(\"10 Flips\")\nplt.hist(flips1)\nplt.axvline(mean1, color='r')\n\nplt.subplot(1,2,2)\nplt.title(\"1000 Flips\")\nplt.hist(flips2)\nplt.axvline(mean2, color='r')\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-1.png){width=658 height=470}\n:::\n:::\n\n\nWhen examining the first graph, with 10 coin flips, we notice the lopsidedness between the outcomes, favoring one side over the other heavily. However, in the second graph, with a 1000 flips, we notice that the bars are about even. Additionally, in the first graph, the mean line is at 0.6, showing it favoring one side over another, but in the second graph, the mean line is at 0.5, showing that it is just about even. This illustrates the Law of Large Numbers, showing that the more data there is, the closer to the assumed mean the data will be.\n\n<br>\n\n**Step 6:**\n\nIllustrate the Central Limit Theorem.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\ndata = np.random.exponential(scale=2, size=10000)\n\nmeans = []\nfor i in range(10, len(data), 30):\n    sample = data[:i]\n    means.append(sample.mean())\n\nplt.plot(means)\nplt.xlabel(\"Sample Size\") \nplt.ylabel(\"Sample Mean\")\nplt.title(\"Convergence of Sample Mean\")\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\nText(0.5, 1.0, 'Convergence of Sample Mean')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-2.png){width=589 height=449}\n:::\n:::\n\n\nAs we see in the graph, as the data points are slowly being summed together over time, we get closer to a normal distribution. There is a very erratic start and it slowly changes to a straighter data set.\n\n<br>\n\n**Step 7:**\n\nThe following code is to visualize the confusion matrix of the data.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n# Prior belief - 90% chance of rain\np_rain = 0.9  \n\n# Likelihood - test is 90% accurate\np_correct = 0.9\n\n# Observation - positive test\np_pos_given_rain = 0.9 \np_pos_given_norain = 0.1\n\n# Posterior - chance of rain given + test   \np_rain_given_pos = (p_pos_given_rain * p_rain) / ((p_pos_given_rain * p_rain) + p_pos_given_norain * (1 - p_rain))\n\nprint(p_rain_given_pos)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.9878048780487805\n```\n:::\n:::\n\n\nAfter analyzing this code, we can see the formula, and data points inserted. According to Bayes' Theorem, if there is 90% chance of rain and the test is 90% accurate, there is a 98.7% chance of rain.\n\n<br>\n\n**Conclusion:**\n\nAs shown throughout this post, the use of Probability Theorems and Random Variables is important in creating predictions and analyzing current data. Through different formulas, graphs, and illustrations, we are able to visualize the different data sets and make educated predictions and assumptions about the data. By using these concepts in Machine Learning, the machine has the ability to implement these formulas and make accurate guesses too. Throughout this blog post, only a few topics were explored, however, there are dozens more out there that all serve their own individual purpose.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}