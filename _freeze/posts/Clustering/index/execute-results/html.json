{
  "hash": "ea86a56233dfda93f08192bc6bd9cbda",
  "result": {
    "markdown": "---\ntitle: \"Topic 2: Clustering\" \nauthor: \"Chiraag Potharaju\" \ndate: \"2023-09-26\" \ncategories: [news, code, analysis] \nimage: \"clustering.png\" \n---\n\nThis post gives detailed descriptions and analysis about Clustering.\n\n<br>\n\n**Introduction to Clustering**\n\nClustering a machine learning technique used to group data points together based on similarities. It can reveal patterns and other common themes to allow accurate predictions and assumptions to be made. Throughout this post, we will dive deeper into probability theory in machine learning using Python, Scikit-learn, and Tensorflow highlighting the importance of this concept. There will also be several data visualizations and executable code chunks to emphasize these points.\n\n<br>\n\n**Important Algorithms and Values**\n\nThere are a some types of algorithms and variables that are important to understand for a basis. Some of them include:\n\n-   K-Means Clustering- Groups data into k number of groups. Efficient for large data sets.\n\n-   Hierarchical Clustering- Builds a hierarchy of clusters using either top-down or bottom-up manners.\n\n-   DBSCAN- Finds clusters based on density. Can detect arbitrary shapes and ignores outliers.\n\n-   Gaussian Mixture Models- Uses a probabilistic model to assume data comes from from a mixture of Gaussian Distributions. Soft clustering.\n\n-   Silhouette coefficient- Quantifies how tightly groups data points are within a cluster to other clusters. Ranges from -1 to 1.\n\n-   Calinski-Harabasz Index- Computes ratio of cluster-to-cluster dispersion, as well as inter cluster dispersion. Higher values claim they are dense.\n\n-   Davies-Bouldin- Calculates average similarity between each cluster and its most similar one. Lower value means more separation between clusters.\n\nThese algorithms and variables are used in several fields such as statistics, machine learning, finance, engineering, medicine, science, and more.\n\n<br>\n\n**Step 0:**\n\nThis step is to ensure that required libraries are installed on the machine before proceeding.\n\n```         \n---   pip install numpy matplotlib scikit-learn ---\n```\n\nThis step installs the required libraries to run the code below. Ensure python is installed on the machine already and the terminal has admin access when running this command.\n\n<br>\n\n**Step 1:**\n\nWhen using Python, we have to import some libraries that will be utilized throughout the Classification process.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.metrics import calinski_harabasz_score\nfrom sklearn.metrics import davies_bouldin_score\nfrom scipy.cluster.hierarchy import linkage\nfrom scipy.cluster.hierarchy import dendrogram\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.mixture import GaussianMixture\n```\n:::\n\n\nSome of the imports include numpy for data manipulation, matplotlib for data visualization, and different sklearn libraries to illustrate different clustering terms.\n\n<br>\n\n**Step 2:**\n\nIllustrate K-Means Clustering.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Generate sample data\nX, y = make_blobs(n_samples=500, n_features=2, centers=4, cluster_std=1.8, random_state=101)\n\n# K-means clustering\nkmeans = KMeans(n_clusters=4, n_init=10)\nkmeans.fit(X)\ny_kmeans = kmeans.predict(X)\n\n# Plot k-means clusters\nplt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\ncenters = kmeans.cluster_centers_\nplt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.9)\nplt.show()\n\n# Evaluate clusters\nprint(\"Silhouette Coefficient: \", silhouette_score(X, y_kmeans))\nprint(\"Calinski-Harabasz Index: \", calinski_harabasz_score(X, y_kmeans)) \nprint(\"Davies-Bouldin Index: \", davies_bouldin_score(X, y_kmeans))\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=577 height=411}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nSilhouette Coefficient:  0.5494869398680785\nCalinski-Harabasz Index:  1384.0339615400558\nDavies-Bouldin Index:  0.6150394374843552\n```\n:::\n:::\n\n\nAs the graph above displays, there are 4 clear groups of data points, as separated by the colors. The silhouette coefficient score of 0.549 indicates that the clusters are well separated and compact. It could be better, but it is still decent. The Calinski-Harabasz Index score of 1384 indicates that data there is good density and separation, as well as good cluster validity. The Davies-Bouldin Index score of 0.615 indicates that that there is moderate separation between clusters with a little overlap. Users can make valid assumptions based on just the scores, however, when looking at the graph, we can see the assumptions made from the scores line up exactly with how the graphs look. These values are very important for statisticians to make their predictions.\n\n<br>\n\n**Step 3:**\n\nIllustrate Hierarchical clustering.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# Generate sample data\nX = [[1,1], [3,4], [4,7], [3,5], [5,6], [6,6], [9,10]]\n\n# Perform hierarchical clustering\nlinked = linkage(X, 'single') \n\n# Plot the dendrogram\ndendrogram(linked,\n            orientation='top',\n            distance_sort='descending',\n            show_leaf_counts=True)\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){width=558 height=414}\n:::\n:::\n\n\nWhen examining the graph above, there are several evident clusters that were created. There are 6 clear clusters that are shown. some data points are very similar, such as 3 and 1 and 4 and 5. However, some data points such as 2 and 6 are way farther apart, showing a lack of similarity between those points. This dendrogram essentially displays how close points are to each other, or similarity, with the distance between the values being the key marker.\n\n<br>\n\n**Step 4:**\n\nIllustrate DBSCAN clustering.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# Generate sample data\nX = np.array([[1, 2], [2, 2], [2, 3],\n              [8, 7], [8, 8], [20,20], [25, 80]])\n\n# Perform DBSCAN clustering\ndbscan = DBSCAN(eps=3, min_samples=2)\ny_dbscan = dbscan.fit_predict(X)\n\n# Plot clusters \nplt.scatter(X[:, 0], X[:, 1], c=y_dbscan)\nplt.title(\"DBSCAN\")\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){width=585 height=449}\n:::\n:::\n\n\nWhen looking at the graph above, we can see several points that are grouped together based on color. The reason that they are grouped together is because of proximity to other points. There are core points there, and the other points are chosen based on their proximity to those points, hence the reason the grouping is as it is.\n\n<br>\n\n**Step 5:**\n\nIllustrate Gaussian Mixture Models.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# Generate sample data\nX = np.array([[1, 2], \n              [1.5, 2.5], \n              [3, 2], \n              [10, 20],\n              [11, 25],\n              [12, 22]])\n\n# Fit Gaussian Mixture Model\ngmm = GaussianMixture(n_components=3)\ngmm.fit(X)\n\n# Predict cluster labels \ny_gmm = gmm.predict(X)\n\n# Plot clusters\nplt.scatter(X[:,0], X[:,1], c=y_gmm, s=40, cmap='viridis')\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-1.png){width=566 height=411}\n:::\n:::\n\n\nWhen analyzing the data points, we can clearly see 3 different colors for the different data points. The yellow group is could be based on the similarity on high y-axis values, the purple with high x-axis values, while the turquoise group is based on low x and y axis values. This graph can be useful to a viewer because if they were adding another point, they can take a valid guess as how it would be grouped.\n\n<br>\n\n**Conclusion:**\n\nAs shown throughout this post, the use of Clustering is important in creating predictions and analyzing current data. Through different algorithms and variables, we are able to visualize the different data sets and make educated predictions and assumptions about the data. There were several graphs that were created, and by analyzing those, we could see the potential of creating predictions for future data points. The practical uses of clustering are endless and it will continue to get more efficient and accurate. By using these concepts in Machine Learning, the machine has the ability to implement these formulas and make accurate guesses too. Throughout this blog post, only a few topics were explored, however, there are dozens more out there that all serve their own individual purpose.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}